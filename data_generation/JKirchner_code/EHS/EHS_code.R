##############################################
#
# Scripts for ensemble hydrograph separation:
#
#     EHS_Fnew    estimates new water fractions by ensemble hydrograph separation, with optional point weights
#
#     EHS_TTD     estimates transit time distribution by ensemble hydrograph separation, with optional volume weighting
#
#     EHS_profile   calculates profiles of Fnew for different percentile ranges of precipitation or discharge



# version 1.4  build 2020.07.15
# Author: James Kirchner, ETH Zurich
#
# Copyright (C) 2020 ETH Zurich and James Kirchner
# Public use of this script is permitted under GNU General Public License 3 (GPL3); for details see <https://www.gnu.org/licenses/>
# 
#
# READ THIS CAREFULLY:
# ETH Zurich and James Kirchner make ABSOLUTELY NO WARRANTIES OF ANY KIND, including NO WARRANTIES, expressed or implied, that this software is
#    free of errors or is suitable for any particular purpose.  Users are solely responsible for determining the suitability and
#    reliability of this software for their own purposes.
#
# ALSO READ THIS:
# These scripts implement the ensemble hydrograph separation approach as presented in J.W. Kirchner, "Quantifying new water fractions
#    and transit time distributions using ensemble hydrograph separation: theory and benchmark tests", Hydrology and Earth System Sciences, 
#    23, 303-349, 2019.
# These scripts are further described in J.W. Kirchner and J.L.A. Knapp, "Calculation scripts for ensemble hydrograph
#    separation", Hydrology and Earth System Sciences, 2020.  
# Users should cite both of these papers (the first for the method, and the second for the scripts).


# Unless otherwise noted, the equation numbers in the scripts refer to the corresponding equations in Kirchner (2019), hereafter denoted K2019. 
# Equation numbers with the designation "KK2020" refer to Kirchner and Knapp (2020). 
# The equations may differ in detail.  For example, in these scripts, k ranges from 1 to m+1 instead of 0 to m 
# due to the array indexing conventions of the R language. 







library(dplyr)  # if dplyr is not already installed, do install.packages("dplyr")
library(matrixStats)  # if matrixStats is not already installed, do install.packages("matrixStats")




##############################################
# define bisquare weight function
##############################################
bisquare <- function(x, MAR) {
  if (MAR==0) w <- ifelse(x==0, 1, 0) # if MAR is zero, only 0 or 1 weights are possible
  else {
    w <- (1-(x/(6*MAR))^2)^2  # bisquare function
    w[x>6*MAR] <- 0  # replace with zero whenever X is more than 6 times the median absolute residual
  }
  return(w)
}




##############################################
# define Welsch weight function
##############################################
Welsch <- function(x, MAR) {
  if (MAR==0) w <- ifelse(x==0, 1, 0) # if MAR is zero, only 0 or 1 weights are possible
  else  w <- exp(-(x/(4.4255*MAR))^2)  # Welsch weight function
  return(w)
}




##############################################
# define Cauchy weight function
##############################################
Cauchy <- function(x, MAR) {
  if (MAR==0) w <- ifelse(x==0, 1, 0) # if MAR is zero, only 0 or 1 weights are possible
  else w <- 1/(1+(x/(3.536*MAR))^2)  # Cauchy weight function
  return(w)
}




##############################################
# Iteratively Reweighted Least Squares (IRLS) with optional fixed point weights (such as water volumes, etc.).
# IRLS iteratively assigns residual weights that are adjusted to downweight data with unusually large residuals.
##############################################
IRLS <- function(Y, X, ww=rep(1,length(Y)), type="Cauchy") {
  
  # Y is a numeric vector representing a response variable
  # X is a numeric vector or array representing one or more explanatory variables
  # ww is an optional numeric vector of point weights (such as masses for mass-weighted regressions)
  #    The default weight vector gives all ww's the value of 1
  # type is an optional string constant indicating the weight function to use.
  #    If type is specified, it must be "bisquare", "Welsch", or "Cauchy". Anything else generates an error.
  #    The default weight function is Cauchy
  
  # Y, ww and X (or each vector comprising X, if X is multi-dimensional) must be of the same length, otherwise an error results.
  # Y and X must not be exactly collinear (that is, there must be some nonzero residuals).  This is not checked.
  
  # IRLS returns a list with two objects:
  # $fit is an object of class "lm", generated by a call lm(Y ~ X, weights=wt, na.action="na.omit")
  #    with the iteratively determined weights. This object can then be handled just like any other 
  #    return from lm, but needs to be prefaced with $fit.  Also, the variable names in this "lm" object 
  #    will be Y and X (as defined internally here) regardless of what names were passed to this function!
  # $wt is the vector of IRLS point weights.
  
  
  
  if (type!="bisquare") {
    if (type!="Welsch") {
      if (type!="Cauchy") stop("IRLS stopped: no valid weight type specified.  Valid functions are 'bisquare', 'Welsch', and 'Cauchy'")
    }
  }
  
  
  if (length(Y) != length(ww)) stop("IRLS stopped: supplied weight vector must be same length as Y")
  
  wt <- rep(1, length(Y)) # initialize residual weights
  
  wt_chg <- 999.0 # initialize weight change
  iter <- 0 # initialize the iteration counter
  
  rsq <- 0.0  # initialize r-squared
  
  ##############################################
  # Here's the IRLS iteration loop, which runs until the largest weight change for any point is less than 0.01, or the iteration limit is exceeded
  ##############################################
  
  while ( (max(wt_chg,na.rm=TRUE) > 0.01) & !all(iter>10, rsq>0.999) ){
    
    if (iter>1000) stop("IRLS stopped: more than 1000 interations, sorry!")
    # This error can arise when Y is perfectly collinear with X for more than half the points,
    # and thus the median absolute residual (MAR) fluctuates near zero, with the weights never stabilizing.
    # That should normally be handled by the r-squared criterion for loop exiting as defined above.
    
    iter <- iter+1 # increment the iteration counter
    old_wt <- wt # save the old vector of weights for comparison with the next one
    
    if (iter==1) {  # start with a line through the median of the data
      
      ymed <- median(Y, na.rm=TRUE) 
      resid <- Y - ymed
      
    } # end of first iteration
    
    else {  # if this is NOT the first iteration
      const <- fit$coefficients[1]   # this is the intercept from lm
      slope <- fit$coefficients[2:length(fit$coefficients)] # this is the vector of regression coefficients from lm
      # explicit calculation of residuals
      if (length(slope)==1) resid <- Y - const - X*slope
      else resid <- as.vector(Y - const - X %*% slope)
      # can't use residuals(fit) because missing values will mess up the assignment of weights in the steps that follow
      # note %*% is matrix multiplication in R
    } # end of else
    
    
    abs_resid <- abs(resid)
    abs_resid_nonzero <- ifelse(abs_resid==0, NA, abs_resid) # exclude residuals corresponding to exact zeroes from median, 
    # to avoid blowup when there are many repeated zeroes
    MAR <- median(abs_resid_nonzero, na.rm=TRUE) # median absolute residual
    
    if (MAR==0.0) stop("IRLS stopped. Solution has collapsed: median absolute residual is zero!")
    
    if (type=="bisquare") wt <- bisquare(abs_resid,MAR) # use bisquare weights
    else if (type=="Welsch") wt <- Welsch(abs_resid,MAR) # use Welsch weights
    else if (type=="Cauchy") wt <- Cauchy(abs_resid,MAR) # use Cauchy weights
    
    wwwt <- ww*wt  # regression weights are user-supplied point weights ww (such as water volumes)
    # multiplied by residual weights wt
    
    fit <- lm(Y ~ X, weights=wwwt, na.action="na.omit") # run multiple regression
    
    rsq <- summary(fit)$r.squared # r-squared
    
    wt_chg <- abs(wt-old_wt) # calculate change in weights from previous iteration
    
  } # end while
  
  qq <- list("fit"=fit, "wt"=wt)  # return lm fit and vector of weights
  
  return(qq)
  
}
##############################################
# END OF iteratively reweighted least squares
##############################################




##############################################
# calculate weighted covariance
##############################################

wtd_cov <- function(x, y, wt=rep(1, length(x)) ){
  # takes as input:
  # x     vector of doubles, potentially with NA values
  # y     vector of doubles, potentially with NA values
  # wt    vector of weights, potentially with NA values
  
  # returns weighted covariance of x and y
  # for weighted variance, do wtd_cov(x, x, wt)
  
  n <- length(x)
  
  # check input length
  if ((length(y) != n) | (length(wt) !=n)) stop("Fatal error: x, y, and wt must have same length in wtd_cov!")
  
  # mask weight values for missing x and y
  # this will have the effect that further calculations will be done on consistent sets of y and x
  wt[ is.na(x) | is.na(y) ] <- NA
  y[ is.na(wt) ] <- NA
  x[ is.na(wt) ] <- NA
  
  if (sum( !is.na(wt) ) < 3) return(NA)  # give up if we have less than 3 non-NA points!
  
  sumwt <- sum( wt, na.rm=TRUE)   # sum of weights
  sumsq <- sum( wt*wt, na.rm=TRUE)   # sum of squared weights
  n.eff <- sumwt*sumwt / sumsq       # effective sample size
  wtd_avg_x <- weighted.mean(x, wt, na.rm = TRUE)  #weighted mean of x
  wtd_avg_y <- weighted.mean(y, wt, na.rm = TRUE)  #weighted mean of y
  
  
  # weighted covariance (equation 56 or 57 -- note that due to a typo there is a factor of Q_j missing in the last summation in both eqs. 56 and 57 in K2019)
  wtd_covar <- (n.eff/(n.eff-1)) * sum( wt*(y - wtd_avg_y)*(x - wtd_avg_x), na.rm=TRUE)/sumwt
  # n.eff/(n.eff-1) is the correction for degrees of freedom
  
  # note that this is equivalent to
  # Galassi, Davies, Theiler, Gough, Jungman, Booth, and Rossi, GNU Scientific Library - Reference manual, Version 1.15, 2011. Sec. 21.7 Weighted Samples
  # https://www.gnu.org/software/gsl/manual/html_node/Weighted-Samples.html#Weighted-Samples, accessed 05 Sept. 2018.
  
  return(wtd_covar)
  
} # end wtd_cov

##############################################
# END OF calculate weighted covariance
##############################################




##############################################
# calculate weighted correlation
##############################################

wtd_cor <- function(x, y, wt=rep(1, length(x))){
  # takes as input:
  # x     vector of doubles, potentially with NA values
  # y     vector of doubles, potentially with NA values
  # wt    vector of weights, potentially with NA values
  
  # returns weighted correlation of x and y
  
  n <- length(x)
  
  # check input length
  if ((length(y) != n) | (length(wt) !=n)) stop("Fatal error: x, y, and wt must have same length in wtd_cor!")
  
  # mask weight values for missing x and y
  # this will have the effect that further calculations will be done on consistent sets of y and x
  wt[ is.na(x) | is.na(y) ] <- NA
  y[ is.na(wt) ] <- NA
  x[ is.na(wt) ] <- NA
  
  if (sum( !is.na(wt) ) < 3) return(NA)
  
  wtd_avg_x <- weighted.mean(x, wt, na.rm = TRUE)  # weighted mean of x
  wtd_avg_y <- weighted.mean(y, wt, na.rm = TRUE)  # weighted mean of y
  
  # weighted correlation
  wtd_correl <- weighted.mean( (y - wtd_avg_y)*(x - wtd_avg_x), wt, na.rm=TRUE )          # wtd covariance without degrees-of-freedom correction
  wtd_correl <- wtd_correl / sqrt( weighted.mean( (x - wtd_avg_x)*(x - wtd_avg_x), wt, na.rm=TRUE) )  # divide by weighted standard deviation
  wtd_correl <- wtd_correl / sqrt( weighted.mean( (y - wtd_avg_y)*(y - wtd_avg_y), wt, na.rm=TRUE) )  # divide by weighted standard deviation
  
  return(wtd_correl)
  
} # end wtd_cor

##############################################
# END OF calculate weighted correlation
##############################################








##############################################
# filter outliers
##############################################

outlier_filter <- function(x) {
  # takes as input:
  # x     vector of doubles, potentially with NA values
  
  # returns x with outliers replaced by NA
  
  medx <- median(x, na.rm=TRUE) # find the median of x
  
  absdev <- abs(x-medx) # absolute value of deviation of x's from medx
  
  MAD <- median(absdev, na.rm=TRUE) # median absolute deviation
  
  x[(absdev>6*MAD)] <- NA # set x values to NA if they deviate from median by more than 6*MAD
  
  # Note that other exclusion criteria besides 6*MAD could be used.  A smaller
  # multiplier on MAD (say, 3*MAD) will raise the breakdown point (the
  # algorithm will be able to cope with larger numbers of outliers) but will
  # also tend to exclude more of the real values and thus will make robust
  # estimation less faithful to the patterns in the real data (depending, of
  # course, on how far the tails of the real data extend).
  
  return(x)   # return filtered x vector
}

##############################################
# END OF outlier filter
##############################################










##############################################
# estimate new water fractions
##############################################

EHS_Fnew <-
  function(Cp,
           Cq,
           P,
           Q = rep(NA, length(Cp)),
           p_threshold = 0,
           vol_wtd = FALSE,
           robust = TRUE, 
           ser_corr = TRUE,
           ptfilter = rep(1, length(Cp)) ) {
    
    
    # takes as input:
    # Cp           vector of tracer concentration in precipitation (at equally spaced points in time)
    # Cq           vector of tracer concentration in streamflow (assumed to be a vector at the same time points at Cp)
    # P            vector of precipitation rate (over the same time intervals as Cp)
    # Q            optional vector of discharge (in same units as precipitation)
    # p_threshold  threshold precipitation rate below which tracer inputs are ignored
    # vol_wtd      logical flag indicating whether volume-weighted new water fractions are desired (TRUE) or not (FALSE)
    # robust       if TRUE, Fnew is estimated by outlier exclusion and iteratively reweighted least squares
    # ser_corr     if TRUE, standard error estimates account for serial correlation in residuals
    # ptfilter     optional vector of flags (1 and 0, T and F, or TRUE and FALSE) indicating whether individual time steps should be included in the analysis
    
    # returns a list as output, with the following objects
    # QpFnew        new water fraction for time steps with precipitation
    # QpFnew_se     standard error of QpFnew
    # QFnew         new water fraction for all time steps, including periods below rainfall threshold
    # QFnew_se      standard error of QFnew
    # PFnew         forward new water fraction (fraction of precipitation that becomes streamflow in the same time step)
    # PFnew_se      standard error of forward Fnew
    #
    #               if vol_wtd = TRUE, QpFnew, QpFnew_se, QFnew, and QFnew_se are discharge-weighted,
    #                                  and PFnew and PFnew_se are precipitation-weighted
    
    
    # Cp and Cq can have missing values (NA).
    # P must exist, and must be non-negative, for all time steps that pass through the point filter (ptfilter==TRUE).
    # Q must either be NA for all time steps that pass through the point filter (ptfilter==TRUE),
    #     or must exist and be be non-negative for all of these points.
    
    # If Q values are NA, then the forward new water fraction will be NA.
    # If Q values are NA and vol_wtd==TRUE, a fatal error will result.
    
    
    # Users should be aware that setting robust = TRUE can potentially give
    # misleading results, if the (many) low-flow points contain relatively small
    # amounts of new water, and the (few) high-flow points have much larger
    # new-water proportions. In such cases, the high-flow points will contain
    # most of the information about new-water contributions to streamflow, but
    # they may be ignored by robust fits because they do not follow the same
    # pattern as the (much more numerous) low-flow points.
    
    
    n <- length(Cp)
    
    # check input data
    if ((length(Cq) != n) |
        (length(P) != n) | 
        (length(Q) != n) | 
        (length(ptfilter) != n) ){
      stop("Fatal error in EHS_Fnew: Cq, Cp, P, Q, and ptfilter must have same length!")
    }
    
    
    # coerce point filter to logical (in case it was supplied as a numeric vector)
    ptfilter <- as.logical(ptfilter)
    
    
    if (sum(is.na(P[ptfilter])) > 0)     # if P is NA for any point that passes through the point filter
      stop("Fatal error in EHS_Fnew: P must not be missing!")        # throw an error
    
    Qfilt <- Q[ptfilter]                  # subset Q to include only points that pass through the point filter
    if (sum(!is.na(Qfilt)) > 0) {         # if any of these Q's are non-NA
      if (min(Qfilt, na.rm = TRUE) < 0)   # then if any are negative
        stop("Fatal error in EHS_Fnew: Q must be >=0!")        # throw an error
      if (sum(is.na(Qfilt)) > 0)          # and if any are NA
        stop("Fatal error in EHS_Fnew: missing values in Q!")  # throw an error
    }
    
    if ( (sum(is.na(Qfilt))>0) & vol_wtd )  # if Q values are missing but we're trying to volume-weight Fnew
      stop("Fatal error in EHS_Fnew: missing Q values in volume-weighted Fnew")  # throw an error
    
    
    
    # for volume-weighted new water fractions, we need to use discharge as weights
    # otherwise we use a vector of 1's as dummy weights
    if (vol_wtd == TRUE) {
      wt <- Q
    } else {
      wt <- rep(1, length(Cp))
    }
    
    # save a copy of the original weights, for future use in rescaling results
    # we need this copy if we do robust estimation, because that will change values of wt
    orig_wt <- wt
    
    # for robust estimation, first exclude extreme values of Cp and Cq (since they could potentially create  
    # x's with leverage that is too strong for robust estimation to overcome)
    if (robust==TRUE) {
      Cp <- outlier_filter(Cp)
      Cq <- outlier_filter(Cq)
    }
    
    
    # create Cq.ref (Cq, lagged by 1) -- reference value of Cq
    Cq.ref <- lag(Cq, 1)     # equation 9
    
    
    Cp[!ptfilter] <- NA  # apply point filter to precipitation concentrations (NA values of Cp will block evaluation of these time steps)
    
    Cq[!ptfilter] <- NA  # apply point filter to discharge concentrations
    
    
    # replace Cp with NA for P less than p_threshold
    Cp[P < p_threshold] <- NA
    
    # now we set up the variables x and y that we will regress to estimate new water fractions
    # create y from Cq and Cq.ref
    y <- Cq - Cq.ref         # equation 9
    
    # create x from Cp and Cq.ref
    x <- Cp - Cq.ref         # equation 9
    
    #mask values of x, y, *and* wt for which x, y, *or* wt have missing values
    y[ is.na(x) | is.na(wt) ] <- NA
    x[ is.na(y) ] <- NA # this also masks any values that were NA in the original y
    wt[ is.na(x) ] <- NA # this also masks any values that were NA in the original y
    
    
    if (robust == TRUE) {
      # robust slope estimation with iteratively reweighted least squares (IRLS)
      
      bb <-
        IRLS(y[!is.na(y)], x[!is.na(x)], wt[!is.na(wt)])  # robust fit via IRLS
      beta_hat <-
        unname(bb$fit$coefficients[2])    # extract the slope coefficient
      
    } else {
      # if we do not want to do robust estimation
      # calculate weighted variance of x    
      var_x <- wtd_cov( x, x, wt )
      
      # create weighted xy covariance vector
      covar_xy <- wtd_cov( x, y, wt )
      
      # now we calculate the regression slope
      beta_hat <-
        covar_xy / var_x   # equation 16
      
    } # end else (non-robust estimation)
    
    # calculating the standard error involves several steps
    # first we need the effective sample size
    
    # effective sample size, accounting for uneven weighting
    # this will equal the non-missing n, if the weights are all equal
    n.eff <- (sum(wt , na.rm = TRUE) ^ 2) / sum(wt * wt , na.rm = TRUE)    # equation 19 (with general weight vector, not necessarily Q)
    
    # Be aware that conventional weighted least squares does not use the
    # effective sample size as calculated here, but rather just the number of
    # non-missing n.  Conventional weighted least squares is designed
    # specifically for situations where different points have different expected
    # uncertainties, and the weights equal the reciprocals of the estimated
    # variances (square of uncertainty) of each point.  The procedure followed
    # here is for situations where the expected uncertainties of the points are
    # the same, but some deserve more weight (for example, so that one obtains
    # an unbiased estimate of a total flux).  In such cases, conventional
    # weighted least squares will underestimate the true uncertainty in the
    # fitted coefficients, and the approach followed here will not.
    
    if (ser_corr == TRUE) { #adjust effective degrees of freedom for serial correlation in residuals
      # to account for serial correlation we need the residuals
      mean_y <- weighted.mean(y, wt, na.rm = TRUE)  # weighted means
      mean_x <- weighted.mean(x, wt, na.rm = TRUE)
      
      e <- y - mean_y - beta_hat * (x - mean_x) # these are the residuals
      
      xe <- e[!is.na(e)]      # collapse the residual vector to retain only non-missing values (but keep them in order)
      xwt <- wt[!is.na(e)]    # collapse the weight vector too
      
      # "Collapsing" the residuals and their weights, like this, is necessary because when there are lots of gaps
      # (for example, when we only analyze high-flow points), there may be very few adjacent pairs of residuals, which
      # will have undue influence on the serial correlation coefficient (calculated below).  Accurately estimating
      # serial correlation (and its effects on uncertainties) requires that we take account of correlations between all
      # the residuals and their lags (that is, the previous residuals), whether or not those pairs occur during adjacent time steps.
      
      #calculate the serial correlation coefficient (with geometric mean weights)
      r_sc <- wtd_cor( xe, lag(xe), sqrt(xwt*lag(xwt,1)) )
      
      #effective degrees of freedom, accounting for serial correlation in residuals
      # Compare with equation 13 of K2019 -- here we apply the serial correlation correction to the degrees of freedom rather than sample size
      df.eff <- (n.eff - 2) * (1.0 - r_sc) / (1.0 + r_sc)  # if ser_corr == TRUE
      
    } else {
      
      df.eff <- n.eff - 2  # if ser_corr == FALSE
      
    } 
    
    # the weighted correlation between x and y
    r_xy <- wtd_cor( x, y, wt)
    
    # now we have all the ingredients for the standard error
    if (df.eff>0) {
      se <- (abs(beta_hat) / sqrt(df.eff)) * sqrt((1 / r_xy ^ 2) - 1)   # equations 11, 15, 20
    } else {
      se <- NA    # if we have negative degrees of freedom, standard error is NA
    }
    
    
    
    # calculate conversion factors
    
    # this conversion factor accounts for rainless periods (equations 14, 15, 18)
    sumwt <- sum(orig_wt[ptfilter], na.rm = TRUE)  # number of time steps, or their total discharge
    if (sumwt == 0) {
      conv_factor <- NA
    } else {     # ratio between number of time steps, or total discharges, with above-threshold precipitation to total.
      conv_factor <- sum(orig_wt[(P >= p_threshold) & ptfilter], na.rm = TRUE) / sumwt 
    }
    
    
    # This factor converts new water fractions to "forward" new water fractions (equations 21 and 28).
    if (!vol_wtd) {                   # if we are not volume-weighting
      if (sum(is.na(Qfilt))>0) {      
        fwd_factor <- NA              # if some Q's are NA, we can't calculate PFnew
      } else {
        fwd_factor <- sum(Q[(P >= p_threshold) & ptfilter], na.rm=TRUE) / sum(P[(P >= p_threshold) & ptfilter], na.rm=TRUE) # (equation 21)
      }
    } else {                          # if we ARE volume-weighting
      fwd_factor <- sum(Q[(P >= p_threshold) & ptfilter], na.rm=TRUE) / sum(P[ptfilter], na.rm=TRUE) # (equation 28)
    }
    
    # Note that the meaning of forward new water fractions differs between the
    # un-weighted (eq. 21) and volume-weighted (eq. 28) cases. If
    # vol_wtd==FALSE, eq. (21) estimates "new" water in streamflow as a fraction
    # of precipitation, averaged over time steps with above-threshold
    # precipitation.  Because this is an unweighted average, it must be
    # restricted to time steps with precipitation because otherwise PFnew is
    # undefined (zero mm of new water divided by zero mm of precipitation). If
    # vol_wtd==TRUE, eq, (28) estimates "new" water in streamflow, averaged over
    # all liters of precipitation (this includes precipitation that falls during
    # below-threshold events). The definition of fwd_factor used here is thus
    # consistent with the fwd_factor used in the EHS_TTD function later in this
    # script.)
    
    
    
    
    return(
      list(
        QpFnew = beta_hat ,                   # new water fraction for time steps with precipitation
        QpFnew_se = se ,                      # standard error
        QFnew = beta_hat * conv_factor ,      #new water fraction for all time steps, including rainless periods
        QFnew_se = se * conv_factor ,         #standard error
        PFnew = beta_hat * fwd_factor ,       # forward new water fraction (fraction of precipitation that becomes same-day streamflow)
        PFnew_se = se * fwd_factor            # standard error
      ) 
    )
    
  }  #end EHS_Fnew

##############################################
# END OF estimate new water fractions
##############################################











##############################################
# make profiles of Fnew
##############################################

EHS_profile <-
  function(Cp,
           Cq,
           P,
           Q = rep(NA, length(Cp)),
           p_threshold = 0,
           ptfilter = rep(1, length(Cp)),
           crit,
           lwr = c(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 95),
           upr = c(20, 30, 40, 50, 60, 70, 80, 90, 100, 100, 100),
           vol_wtd = FALSE,
           robust = TRUE,
           ser_corr = TRUE)

  {
    
    # takes as input:
    # Cp           vector of tracer concentration in precipitation (at equally spaced points in time)
    # Cq           vector of tracer concentration in streamflow (assumed to be a vector at the same time points at Cp)
    # P            vector of precipitation rate (over the same time intervals as Cp)
    # Q            optional vector of discharge (in same units as precipitation)
    # p_threshold  threshold precipitation rate below which tracer inputs are ignored
    # ptfilter     optional vector of flags (1 and 0, T and F, or TRUE and FALSE) indicating whether individual time steps 
    #                   should be included in the analysis.  Note that ptfilter is redefined internally before being passed to EHS_Fnew
    # crit         vector containing criterion for subsetting (for example, Q or P)
    # lwr          lower bounds of subsets in percentile ranks
    # upr          upper bounds of subsets in percentile ranks
    # vol_wtd      logical flag indicating whether volume-weighted new water fractions are desired (TRUE) or not (FALSE)
    # robust       if TRUE, Fnew is estimated by outlier exclusion and iteratively reweighted least squares
    # ser_corr     if TRUE, standard error estimates account for serial correlation in residuals
    
    # EHS_profile divides the input time series into length(lwr) (=length(upr))
    # subsets, for which the percentile rank of crit is between lwr and upr.  It
    # then runs EHS_Fnew on each of these subsets and returns vectors of the
    # results.
    
    # returns a list as output, with the following vector objects
    # bin_avg      bin averages of criterion variable
    # QpFnew        new water fraction for time steps with precipitation
    # QpFnew_se     standard error of QpFnew
    # QFnew         new water fraction for all time steps, including periods below rainfall threshold
    # QFnew_se      standard error of QFnew
    # PFnew         forward new water fraction (fraction of precipitation that becomes streamflow in the same time step)
    # PFnew_se      standard error of forward Fnew
    #
    #               if vol_wtd = TRUE, QpFnew, QpFnew_se, QFnew, and QFnew_se are discharge-weighted,
    #                                  and PFnew and PFnew_se are precipitation-weighted
    
    
    # Cp and Cq can have missing values (NA)
    # P  must not have missing values
    # Q can be omitted, but in that case the forward Fnew and all weighted Fnew's will be NA
    # crit must be same length as vectors of Cp, Cq, P, and Q
    
    # Be aware that in some circumstances, profiles using P as a criterion can yield strange results.  
    # Specifically, filtering to include small values of P can lead to small and
    # uncertain values of QpFnew, and in some cases to average Q >> average P
    # (because Q can be supplied by P in previous time steps). Then the
    # conversion to PFnew will multiply by the large ratio of average Q/average
    # P, leading to large and highly uncertain values of PFnew.
    
    
    
    if (length(upr)!=length(lwr)) {
      stop("Fatal error in EHS_profile: upper and lower threshold vectors must be same length!")
    }
    
    if (min(upr-lwr)<=0) {
      stop("Fatal error in EHS_profile: upper thresholds must be above lower thresholds!")
    }
    
    # convert the point filter to logical
    ptfilter <- as.logical(ptfilter)
    
    # now copy the filter criterion
    crit_copy <- crit
    
    # We need to filter the filter criterion itself, to exclude unusable points.  These include
    # points with precipitation below the precipitation threshold, or with missing Cp and Cq.
    crit[(P<p_threshold)] <-NA
    crit[(is.na(Cp))] <-NA
    crit[(is.na(Cq))] <-NA
    crit[(is.na(P))] <- NA
    crit[!ptfilter] <- NA    # if the input series has been filtered (by supplying ptfilter as input), apply that filter here
    
    # if we are volume-weighting, we also need to filter out criterion values for which discharge is unavailable
    if (vol_wtd) { crit[is.na(Q)] <-NA }
    
    # Note that filtering the criterion variable in this way gives us (say) x%
    # of the distribution of potentially usable points (since, for example,
    # P<p_threshold will typically not be used).  Otherwise we risk (for
    # example) a case where a range of low discharges have few (or zero!) points
    # that have above-threshold P, and thus Cp, with which to calculate Fnew.
    
    # Note, however, that while this approach guarantees a reasonable number of
    # usable points in each range of percentages, but it also means that those
    # ranges will not match the raw percentiles of (for example) P or Q.
    
    
    # convert the percentages in lwr and upr to threshold values of the criterion variable
    lwr <- quantile(crit, probs=lwr/100.0, na.rm=TRUE, type = 5)
    upr <- quantile(crit, probs=upr/100.0, na.rm=TRUE, type = 5)
    
    
    # now we need to retrieve our copy of the original criterion vector
    crit <- crit_copy
    
    for(i in 1:length(lwr)) {  # step through levels of the filters
      
      # Here we re-define the point filter
      ptfilter <- rep(1, length(Cp))
      ptfilter[is.na(crit)] <- 0    # this is redundant given that NA values of crit will trigger both of the next tests anyhow
      ptfilter[(crit<lwr[i])] <- 0  # exclude values below lower threshold
      ptfilter[(crit>upr[i])] <- 0  # exclude values above upper threshold
      
      
      # calculate new water fractions using the filtered data
      Fnew <- EHS_Fnew(Cp, Cq, P, Q, p_threshold=p_threshold, ptfilter=ptfilter, vol_wtd=vol_wtd, robust=robust, ser_corr=ser_corr)
      
      # average criterion value within this bin
      binmean <- mean(crit[as.logical(ptfilter)], na.rm=TRUE)
      
      # average criterion value within this bin for points with above-threshold precipitation
      binmean_above_threshold <- mean(crit[as.logical(ptfilter) & (P>=p_threshold)], na.rm=TRUE)
      
      # make the results into a new row of a data frame
      newrow <- as.data.frame(c(lwr_limit=as.numeric(lwr[i]),           # lower limit of critieron
                                upr_limit=as.numeric(upr[i]),           # upper limit of criterion
                                bin_avg=binmean,                        # average criterion value for points within bin
                                bin_avg_with_P=binmean_above_threshold, # average criterion value for points with above-threshold precipitation
                                Fnew))                                  # new water fraction results for bin-filtered points
      
      if (i==1) results <- newrow  # if this is the first row, start the "results" data frame
      else results <- rbind(results, newrow) # otherwise append it to the existing one (this is inefficient but that doesn't matter)
      
    } #next i
    
    return(as.list(results))
    
  }  #end EHS_profile

##############################################
# END OF make profiles of Fnew
##############################################















##############################################
# calculate weighted covariance matrix
##############################################

wtd_covar_matrix <- function(X, wt=rep(1, length(X))){
  # takes as input:
  # x     matrix of doubles, potentially with NA values
  # wt    vector of weights, potentially with NA values
  
  # returns weighted covariance matrix, with each element equalling the covariance between the corresponding columns of X
  
  n <- nrow(X)
  m <- ncol(X)
  
  # check input length
  if (length(wt) !=n) stop("Fatal error: x and wt must have same length in wtd_covar_matrix!")
  
  C = matrix(NA, nrow=m, ncol=m) # start with a blank matrix
  
  for (j in 1:m) for (k in 1:m) {
    if (j<=k) C[j,k] <- wtd_cov(X[,j], X[,k], wt)  # call the weighted covariance routine, passing the relevant columns of X
    else C[j,k]<-C[k,j] # do the lower triangle by symmetry
  }
  
  return(C)
  
} # end wtd_covar_matrix

##############################################
# END OF calculate weighted covariance matrix
##############################################




##############################################
# create regularization matrix
##############################################

tikh <- function(m) {
  # creates m x m Tikhonov - Phillips regularization matrix (equation 49)
  
  x <- matrix(0L, nrow = m, ncol = m) # create matrix filled with zeroes
  
  # put values on diagonal
  x[1, 1] <- 1
  x[m, m] <- 1
  x[2, 2] <- 5
  x[m - 1, m - 1] <- 5
  for (i in 3:(m - 2)) {
    x[i, i] <- 6
  }
  
  # put values in innermost off-diagonal rows
  x[1, 2] <- -2
  x[2, 1] <- -2
  x[m, m - 1] <- -2
  x[m - 1, m] <- -2
  for (i in 3:(m - 1)) {
    x[i, i - 1] <- -4
    x[i - 1, i] <- -4
  }
  
  # put values in outermost off-diagonal rows
  for (i in 3:m) {
    x[i, i - 2] <- 1
    x[i - 2, i] <- 1
  }
  
  return(x)
} # end tikh

##############################################
# END OF create regularization matrix
##############################################
















##########################################################################
# estimate transit time distributions, with filtering and volume-weighting
##########################################################################

EHS_TTD <-
  function(Cp ,
           Cq ,
           P ,
           Q = rep(NA, length(Cp)),
           p_threshold = 0 ,
           m = 10 ,
           nu = 0.5 ,
           Pfilter = rep(1, length(P)) ,
           Qfilter = rep(1, length(Q)) ,
           vol_wtd = FALSE ,
           robust = TRUE ,
           ser_corr = TRUE ) {
    
    
    # takes as input:
    # Cp           vector of tracer concentration in precipitation (at equally spaced points in time)
    # Cq           vector of tracer concentration in streamflow (assumed to be a vector at the same time points as Cp)
    # P            vector of precipitation rate (over the same time intervals as Cp)
    # Q            optional vector of discharges (in same units as P)
    # p_threshold  threshold precipitation rate below which tracer inputs are ignored
    # m            maximum lag in TTD (in number of time steps)
    # nu           fractional weight given to Tikhonov-Phillips regularization
    # Pfilter      optional vector of flags (1 and 0, T and F, or TRUE and FALSE) indicating whether individual precipitation time steps should be included in the analysis
    # Qfilter      optional vector of flags (1 and 0, T and F, or TRUE and FALSE) indicating whether individual discharge time steps should be included in the analysis
    # vol_wtd      logical flag indicating whether volume-weighted TTD's are desired (TRUE) or not (FALSE)
    # robust       if TRUE, TTD estimation employs outlier exclusion and iteratively reweighted least squares
    # ser_corr     if TRUE, standard error estimates account for serial correlation in residuals
    
    # returns a list as output, with the following objects (each is a vector of length m+1, to include a lag of zero time steps)
    # TTD          transit time distribution (if vol_wtd==TRUE, this TTD is weighted by discharge)
    # TTD_se       standard error of TTD
    # fwd_TTD      forward transit time distribution (if vol_wtd==TRUE, this TTD is weighted by precipitation)
    # fwd_TTD_se   standard error of forward TTD
    
    
    # Cp and Cq can have missing values (NA).
    # P must not have missing values.  The same is true for Q, if it is present (that is, all or none of Q values must be NA).
    # Q can be omitted, but if so, the forward transit time distribution will be NA, and all flow-weighted TTD's will be NA.
    # Values of P (and Q, if present) must be non-negative.
    
    
    # IMPORTANT NOTE: This calculation proceeds differently from the one presented in K2019 in several key respects.
    #
    # Specifically, instead of excluding x values for missing concentrations (and those corresponding to below-threshold precipitation
    # amounts) from the calculations of covariances, this calculation replaces them by zeroes (after the weighted mean is subtracted from
    # each x variable).  This means that the missing values are no longer excluded, but instead are replaced by points that have no 
    # leverage (since the variables are all centered around zero).  This simplifies the overall calculation scheme somewhat, and also
    # allows explicit calculation of residuals and residual variance (for estimating parameter uncertainties).  The overall approach
    # remains the same, but the missing values must be taken into account differently, since replacing a missing value with zero dilutes the
    # calculated covariances (which is correct for below-threshold rainfall, but not for missing measurements), whereas excluding a missing
    # value inflates the covariances (in the case of below-threshold rainfall), but doesn't inflate the covariances for missing measurements.
    #
    # In practice this means that the calculations proceed similarly, but n.xk and n.xkxl must be defined differently and must be handled
    # differently.  In K2019, n.xk and n.xkxl are the numbers of x's that have above-threshold precipitation (and therefore should
    # be part of the covariance calculation).  Here, instead, we count the number (or weight) of "usable" x[,k] values or x[,k]x[,l] pairs, 
    # and denote these u.xk and u.xkxl (note the change in notation), where u stands for "number usable".  For an x[j,k] value to be "usable",
    # either P must be below the threshold (in which case it doesn't matter whether Cp is missing or not, because it shouldn't contribute 
    # significantly to the covariances), or Cp[i=j-k] and C.ref[j] must both exist (so that the contribution to the covariance can be calculated).
    # 
    
    # Note also that the sampling interval delta_t is not explicitly represented here, as it is in Eqs. (55), (60), (63), and (66) of K2019.
    # This means that the TTD's should be interpreted as a fraction of discharge (or precipitation) per sampling interval (i.e., per day if the sampling
    # interval is one day, per week if the sampling interval is one week, etc.)
    
    # The average time lag between precipitation and streamflow for each lag index k will be (k-0.5)*delta_t (for grab sampling of streamflow)
    # or (k-0.75)*delta_t (for time-averaged sampling of streamflow).  These formulas differ from those in Sect. 4.1 of K2019 because
    # here, the index k starts at 1 rather than 0.
    
    # In addition, filtering by precipitation time steps proceeds fundamentally differently than in K2019.  
    # If any Pfilter==FALSE (filtering by precipitation time), we divide the input time series into
    # two subsets (for Pfilter true and false), and analyze them jointly.  
    # The subsets must be analyzed jointly, each with its own TTD coefficients, because
    # the lagged precipitation inputs from each subset must be interleaved (since the "excluded"
    # points will still affect the streamflow concentrations, and their effects must be accounted for in the 
    # regression model (just not with the same coefficients).
    
    
    
    
    
    #########################################
    # check input data
    
    n <- length(Cp) # n is total number of rows in our problem
    
    if ((length(Cq) != n) | (length(P) != n) | (length(Q) != n) | (length(Pfilter) != n) | (length(Pfilter) != n)) {
      stop("Fatal error in EHS_TTD: Cq, Cp, P, Q, Pfilter, and Qfilter must have same length!")
    }
    
    if (sum(is.na(Pfilter))>0) stop("Fatal error in EHS_TTD: missing values in Pfilter!")
    if (sum(is.na(Qfilter))>0) stop("Fatal error in EHS_TTD: missing values in Qfilter!")
    
    
    if (min(P) < 0)
      stop("Fatal error in EHS_TTD: P must be >= 0!")
    if (sum(is.na(P)) > 0)
      stop("Fatal error in EHS_TTD: missing values in P!")
    if (sum(!is.na(Q)) > 0) {
      if (min(Q, na.rm = TRUE) < 0)
        stop("Fatal error in EHS_TTD: Q must be >=0!")
      if (sum(is.na(Q)) > 0)
        stop("Fatal error in EHS_TTD: missing values in Q!")
    }
    
    # if we will be volume-weighting, then the weight vector is Q, otherwise it is just a set of 1's
    if (vol_wtd == TRUE) {
      wt <- Q / mean(Q, na.rm=TRUE)  #normalize weights to mean of 1 (this is just good numerical hygiene)
    } else {
      wt <- rep(1, n)
    }
    
    
    # for robust estimation, exclude extreme values of Cp and Cq (since they could otherwise create multivariate 
    # x's with leverage that is too strong for robust estimation to overcome)
    if (robust==TRUE) {
      Cp <- outlier_filter(Cp)
      Cq <- outlier_filter(Cq)
    }
    
    # coerce Pfilter and Qfilter to logical (in case they were supplied as numeric vectors)
    Pfilter <- as.logical(Pfilter)
    Qfilter <- as.logical(Qfilter)
    
    
    # replace Cp with NA for P<p_threshold
    Cp[P < p_threshold] <- NA
    
    # create Cq.ref (reference value of Cq, lagged one more than maximum TTD lag)
    Cq.ref <-
      lag(Cq, (m + 1))                               # equation 34
    
    
    
    
    #########################################
    # calculate y
    
    # create y from Cq and Cq.ref
    y <- Cq - Cq.ref                             # equation 35
    
    
    
    #########################################
    # calculating x matrix is special!
    
    if (sum(!Pfilter)>0) filterP <- TRUE    else    filterP <- FALSE     # a flag to indicate whether we will be filtering by precipitation time or not
    if (filterP==TRUE) n_sets <- 2    else    n_sets <- 1     # if we are filtering by precipitation time, we have two sets of inputs 
    # we need to account for: those included by the filter, and those excluded by it
    mm <- n_sets*(m+1)            # mm is the width of the x matrix (and the total number of fitted coefficients)
    
    x <- matrix(NA, nrow=n, ncol=mm)  # initialize the x matrix (double-wide if we are filtering by precipitation time)
    
    
    if (filterP==FALSE) {                                   # if we are not filtering by precipitation
      for (k in 1:(m + 1))
        x[, k] <- lag(Cp, (k - 1)) - Cq.ref                 # equation 35
      
    } else {
      
      z <- rep(NA, n)
      for (k in 1:(m + 1)) {
        lagPfilter <- lag(Pfilter, k-1)
        xx <- lag(Cp, (k - 1)) - Cq.ref
        x[, k] <- ifelse( lagPfilter, xx, z)                      # "in set" precipitation values (with NA's in-filled for out-of-set values)
        x[, (m+1)+k] <- ifelse( !lagPfilter, xx, z)               # "out of set" precipitation values (with NA's in-filled for in-set values)
      }                                                           # Note that we must in-fill with NA's, not zeroes, because these are missing values
    }                                                             # (not known concentrations of zero -- *flux* is zero but concentration is NA).
    # These will eventually be replaced with zeroes after centering the known values.
    
    
    
    ##############################################################################################
    # create matrix of boolean flags indicating whether each x value corresponds to above-threshold precipitation or not (whether or not Cp is missing)
    
    above_thresh <- matrix(NA, nrow=n, ncol=(n_sets*(m+1)))  # initialize the above threshold matrix (double-wide if we are filtering by precipitation time)
    
    if (filterP==FALSE) {                                   # if we are not filtering by precipitation
      for (k in 1:(m + 1))
        above_thresh[, k] <- (lag(P, (k - 1)) >= p_threshold) 
    } else {
      z <- rep(FALSE, n)
      
      for (k in 1:(m + 1)) {
        
        lagPfilter <- lag(Pfilter, k-1)
        xx <- (lag(P, (k - 1)) >= p_threshold)
        above_thresh[, k] <- ifelse( lagPfilter, xx, z)                      # "in set" values (with FALSE in-filled for out-of-set values)
        above_thresh[, (m+1)+k] <- ifelse( !lagPfilter, xx, z)               # "out of set" values (with FALSE in-filled for in-set values)
        
      }
    }
    
    
    
    ##############################################################################################
    # create matrix of boolean flags indicating usable x values (where Cp and C.ref both exist, or P is below threshold and we can safely set x to zero below) 
    
    usable <- matrix(NA, nrow=n, ncol=(n_sets*(m+1)))  # initialize the "usable" matrix (double-wide if we are filtering by precipitation time)
    
    for (k in 1:(m + 1))
      usable[, k] <- (!is.na(x[,k]) |  !above_thresh[,k])   # equation A6 of KK2020
    if (filterP==TRUE) 
      for (k in (m + 2):mm)
        usable[, k] <- (!is.na(x[,k]) |  !above_thresh[,k])   # equation A6 of KK2020
    
    
    
    
    # from here on, we will work with x and y, rather than with the original Cp and Cq
    
    
    #########################################
    # remove missing rows, center columns on weighted means, and in-fill individual missing x's with zeroes (so that they will have no leverage)
    
    # here we apply Qfilter by setting y to NA (this will trigger the deletion of the entire row)
    y[!Qfilter] <- NA
    
    # set y to NA if all x's are NA (since we won't be able to do anything with this row anyhow)
    for (j in 1:n) if (sum(!is.na(x[j,]))==0) y[j] <- NA
    
    # delete rows of x where y is missing (since we won't be able to do anything with these anyhow)
    x <- x[!is.na(y),]
    
    # delete rows of "usable" where y is missing (since we won't be able to do anything with these anyhow)
    usable <- usable[!is.na(y),]
    
    # also delete weights where y is missing
    ww <- wt[!is.na(y)]            # note name change to ww
    
    # and delete missing elements of y (otherwise the rows of y won't match the rows of x...!)
    y <- y[!is.na(y)]
    
    # subtract weighted mean from y's 
    y <- y - weighted.mean(y, ww, na.rm=TRUE)
    
    # subtract weighted means from x's
    meanx <- colWeightedMeans(x, w=ww, na.rm=TRUE)
    x <- sweep(x, 2, meanx) # sweep function subtracts the column means from each row of x
    
    # replace missing values of x with zeroes (remember that these should have little leverage because we have already centered each of the x's)
    x[is.na(x)] <- 0
    
    
    #########################################
    # optionally, calculate robustness weights
    
    if (robust==TRUE) {
      
      robustfit <- IRLS(y, x, ww=ww) # iteratively reweighted least squares -- we don't use this to fit the TTD, but just to 
      # downweight any rows with outliers
      
      ww <- ww*robustfit$wt          # multiply by robustness weights (which will be small for outliers)
      
    } 
    
    
    
    #########################################
    # set up weighted linear system
    
    # create weighted covariance matrix
    covar <- wtd_covar_matrix(x , ww)            # equation A7 of KK2020
    
    # create weighted xy covariance vector
    xy <- rep(NA, mm)
    for (k in 1:mm) xy[k] <- wtd_cov(x[,k], y, ww)  # equation A8 of KK2020
    
    # now we need to weight the elements of the matrix by u.xkxl/u.xk (analogous to Appendix B of the paper, but with modifications as described below)
    # These are defined in equation A10 of KK2020
    
    # calculate u.xk as vector of total count (or total weight) of x's that have usable P values (analogous to equation 45, but with u rather than n)
    # note that "usable" means that P is below threshold or Cp and Cq.ref are both not missing
    u.xk <- rep(NA, mm) # start with a vector of NA
    for (k in 1:mm) u.xk[k] <- sum(ww[usable[,k]], na.rm=TRUE)  # fill with sum of weights for x's that are not unusable
    # usable means that any x of zero should contribute zero to covariance: P is below threshold or Cp and Cq.ref are not missing
    
    # calculate u.xkxl as matrix of total count (or total weight) with non-missing values for each lag in both xk and xl (analogous to equation 45, but with u rather than n)
    u.xkxl <- matrix(NA, nrow = mm, ncol = mm)
    for (k in 1:mm) {
      for (l in 1:mm) {
        u.xkxl[k, l] <- sum(ww[(usable[,k] & usable[,l])], na.rm=TRUE)  # fill with sum of weights that are usable in both x_k and x_l
        # usable means that any x of zero should contribute zero to covariance: P is below threshold or Cp is not missing
      }
    } 
    
    
    if (sum(is.na(u.xk)) > 0)
      stop("problem with u.xk")
    if (sum(is.na(u.xkxl)) > 0)
      stop("problem with u.xkxl")
    if (sum(is.na(covar)) > 0)
      stop("problem with C matrix")
    
    # Now adjust covariance matrix by both u.xk and u.xkxl 
    # NOTE, however, that we need to do this BACKWARDS compared to the way it is done in K2019, because in this implementation
    # the missing values are replaced by zeroes, and thus dilute the covariances (they contribute nothing to the cross-products, but their
    # number or weight is counted in the denominator).  This is correct if the missing values correspond to below-threshold precipitation
    # (those *should* dilute the covariances), but if precipitation is above the threshold and the concentration is missing for other
    # reasons, we need to correct for the dilution of the covariance.  
    
    # To correct for this dilution we we need to re-scale each element of 
    # the covariance matrix "covar" by the ratio between the total number (or weight) and u.xkxl, and we need to re-scale each element of
    # the covariance array xy by the total number (or weight) and u.xk.  Equivalently, we can just re-scale each element of the covariance 
    # array covar by u.xk/u.xkxl (the reciprocal of what was used in K2019, where all missing values were excluded from
    # the covariance calculations, rather than being replaced with zeroes).
    
    
    C <- covar * u.xk / u.xkxl    # element-wise multiplication and division, not matrix operations!  (equation A11 of KK2020)
    
    #######################################################################
    # now we solve the regularized (and weighted) linear regression problem
    
    # create regularization matrix
    H <- tikh(m + 1)
    
    # If we are filtering by precip time, we can't use one big regularization matrix for the whole problem, 
    # because it will lead to leakage between the end of one TTD and the start of another.
    # Here instead we use a segmented regularization matrix, with two
    # Tikhonov-Phillips matrices along the diagonal.  This isolates the regularization
    # effects within each block of (m+1) coefficients.
    if (filterP) {
      toprow <- cbind(H, matrix(0, nrow=(m+1), ncol=(m+1)))
      bottomrow <- cbind(matrix(0, nrow=(m+1), ncol=(m+1)), H)
      H <- rbind(toprow, bottomrow)
    }
    
    
    lambda <- nu / (1.0 - nu) * sum(diag(C)) / sum(diag(H))              # equation 50
    
    # now solve (regularized) linear system to get TTD coefficients      # equation 46, and equation A12 of KK2020
    beta_hat <- solve((C + (lambda * H)), xy)
    
    
    
    
    
    
    #########################################
    # estimate standard errors
    
    # Here we do not use Glasser's error variance estimate (equation 52) because it is not particularly reliable.
    # Instead we calculate the error variance directly from residuals of y for each x.
    # Because we have replaced the missing x's with zeroes, we can still estimate y (for which we need values for each x) without giving the 
    # missing points too much influence (this works because the x's have already been "centered" by subtracting their weighted means).
    
    e <- y - x %*% beta_hat  # these are the residuals  (equation A13 of KK2020)
    # (note that means have already been subtracted from y and the x's, so there is no intercept)
    
    nn <- length(e)          # number of rows in our problem (note that this is not n, the number of original data points, because some have been removed)
    
    s2e <- (nn-1)/(nn-mm-1) * wtd_cov(e, e, ww)  # weighted variance of residuals, corrected for degrees of freedom (equation A14 of KK2020)
    # note wtd_cov already corrects for uneven weighting 
    
    
    # first we need to calculate the effective sample size 
    # (note that we do *not* use Eq. 59 of K2019 because uneven weighting is already taken into account in s2e above)
    n.eff <- rep(1, mm)  # define the vector
    for (k in 1:mm) {
      n.eff[k] <- sum(usable[,k], na.rm=TRUE)     # count usable points    (equation A17 of KK2020)
    }
    
    
    if (ser_corr == TRUE) { # adjust sample size for serial correlation in residuals
      
      #calculate the serial correlation coefficient (with geometric mean weights)
      r_sc <- wtd_cor( e, lag(e), sqrt(ww*lag(ww,1)) )
      
      #effective sample size, accounting for serial correlation in residuals
      n.eff <- n.eff * (1.0 - r_sc) / (1.0 + r_sc)  # equation A16 of KK2020
      
    } 
    
    
    # now we need to estimate the inverse of the regularized covariance matrix
    regC.inverse = solve((C + (lambda * H))) # inverse of regularized C matrix
    
    # now calculate the parameter covariance matrix
    beta_covar_matrix = (s2e / n.eff) * regC.inverse %*% C %*% regC.inverse # covariance matrix of coefficients (equation A15 of KK2020)
    
    # standard error is the square root of the diagonal of this matrix (equation 54)
    # unless degrees of freedom are zero or negative, in which case assign NA
    
    dd <- diag(beta_covar_matrix)  # diagonal of the matrix
    
    se <- rep( NA, mm)   # initialize standard error as NA
    
    for (k in 1:mm) if (dd[k]>0) se[k] <- sqrt(dd[k])
    # we can't do this with ifelse, because that will trigger a warning if any of the dd are negative
    
    
    
    
    
    #########################################
    # calculate conversion factors
    
    # we need this conversion factor, conv_factor, to account for rainless periods (equations 55 and 60)
    # note that this uses wt rather than ww, which has been altered by deletion of rows from our problem
    conv_factor <- rep(NA, mm)
    
    for (k in 1:(m+1))                   # here we have to account for the possibility that there has been filtering by both P and Q...!
      conv_factor[k] <- sum( wt[above_thresh[,k] & Qfilter & lag(Pfilter, k-1 )], na.rm=TRUE ) / sum( wt[Qfilter & lag(Pfilter, k-1 )], na.rm=TRUE ) 
    if (filterP)           # if we are filtering by precipitation time steps
      for (k in 1:(m+1))   # note that here we need to reverse Pfilter (because we are counting the "out of set" values)
        conv_factor[k+m+1] <- sum( wt[above_thresh[,k+m+1] & Qfilter & lag(!Pfilter, k-1 )], na.rm=TRUE ) / sum( wt[Qfilter & lag(!Pfilter, k-1 )], na.rm=TRUE )
    
    
    # we need this conversion factor, fwd_factor, to convert TTD to forward TTD (equations 63, 64, 65, 66)
    # note that here, again, we have to account for the possibility that there has been filtering by both P and Q...!
    fwd_factor <- rep(NA, mm)
    
    if (!vol_wtd) {    # not volume-weighted: equations 63, 64, 65
      for (k in 1:(m+1))
        fwd_factor[k] <- mean( Q[(above_thresh[,k] & Qfilter & lag(Pfilter, k-1 ))] , na.rm = TRUE ) / mean(P[(above_thresh[,1] & lead(Qfilter, k-1) & Pfilter)], na.rm = TRUE )
      if (filterP)          # if we are filtering by precipitation time steps
        for (k in 1:(m+1))  # note that here we need to reverse Pfilter (because we are counting the "out of set" values)
          fwd_factor[k+m+1] <- mean( Q[(above_thresh[,k+m+1] & Qfilter & lag(!Pfilter, k-1 ))] , na.rm = TRUE )/ mean(P[(above_thresh[,m+2] & lead(Qfilter, k-1) & !Pfilter)], na.rm = TRUE )
      
    } else {           # volume-weighted: equation 66 (with conv_factor included)
      
      for (k in 1:(m+1))
        fwd_factor[k] <- conv_factor[k] * mean( Q[(Qfilter & lag(Pfilter, k-1 ))] , na.rm = TRUE ) / mean(P[(lead(Qfilter, k-1) & Pfilter)], na.rm = TRUE )
      if (filterP)    # if we are filtering by precipitation time steps
        for (k in 1:(m+1))   # note that here we need to reverse Pfilter (because we are counting the "out of set" values)
          fwd_factor[k+m+1] <- conv_factor[k+m+1] * mean( Q[(Qfilter & lag(!Pfilter, k-1 ))] , na.rm = TRUE ) / mean(P[(lead(Qfilter, k-1) & !Pfilter)], na.rm = TRUE )
      
    } # end if (vol_wtd)
    
    
    
    
    #########################################
    # return results
    
    beta_hat <- as.vector(beta_hat)  # type cast
    
    
    TTD = beta_hat * conv_factor            # restricted range to throw away "out of sample" coefficients if Pfilter==TRUE
    # transit time distribution (corrected for rainless periods)
    TTD_se = se * conv_factor
    # standard error of TTD
    fwd_TTD = beta_hat * fwd_factor
    # forward TTD
    fwd_TTD_se = se * fwd_factor
    # standard error of forward TTD
    
    
    return(
      list(                                                         # restricted range to throw away "out of sample" coefficients if filterP==TRUE
        TTD = beta_hat[1:(m+1)] * conv_factor[1:(m+1)] ,            # transit time distribution (corrected for rainless periods)
        TTD_se = se[1:(m+1)] * conv_factor[1:(m+1)] ,               # standard error of TTD
        fwd_TTD = beta_hat[1:(m+1)] * fwd_factor[1:(m+1)] ,         # forward TTD
        fwd_TTD_se = se[1:(m+1)] * fwd_factor[1:(m+1)]              # standard error of forward TTD
      )
    ) # end return
    
  } # end EHS_TTD


##############################################
# END OF estimate transit time distributions
##############################################



